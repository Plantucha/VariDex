#!/usr/bin/env python3
"""
varidex/utils/helpers.py - Helper Components v6.0.0
====================================================
Reusable validation, progress tracking, and classification helpers for
the VariDex ACMG variant classification pipeline.

Components:
- DataValidator: DataFrame and variant validation
- OptimizedProgressTracker: Memory-efficient progress bar with ETA
- classify_variants_production: Batch classification with safeguards

Reference: Richards et al. 2015 (ACMG Guidelines), PMID 25741868
"""

import time
import logging
from pathlib import Path
from typing import Dict, List, Tuple, Optional, TYPE_CHECKING
from collections import deque

if TYPE_CHECKING:
    import pandas as pd

from varidex.version import __version__
from varidex.core.models import VariantData, ACMGEvidenceSet
from varidex.core.genotype import GenotypeNormalizer
from varidex.core.classifier import ACMGClassifier
from varidex.exceptions import ValidationError, VaridexError

logger = logging.getLogger(__name__)


# ============================================================================
# DATA VALIDATION
# ============================================================================

class DataValidator:
    """Unified validation for DataFrames and variants."""

    @staticmethod
    def validate_dataframe_structure(
        df: "pd.DataFrame",
        stage_name: str,
        required_columns: List[str]
    ) -> Tuple[bool, Optional[str]]:
        """
        Validate DataFrame structure and required columns.

        Returns:
            (is_valid, error_message): Tuple indicating validation status
        """
        import pandas as pd

        if df is None:
            return False, f"{stage_name}: DataFrame is None"

        if len(df) == 0:
            return False, f"{stage_name}: DataFrame is empty"

        missing = set(required_columns) - set(df.columns)
        if missing:
            return False, f"{stage_name}: Missing columns: {missing}"

        # Check for excessive NaN (>95% threshold)
        for col in required_columns:
            if col in df.columns:
                nan_pct = df[col].isna().sum() / len(df)
                if nan_pct > 0.95:
                    return False, f"{stage_name}: Column '{col}' is {nan_pct*100:.1f}% NaN"

        return True, None

    @staticmethod
    def validate_variant(
        variant: VariantData,
        idx: int
    ) -> Tuple[bool, Optional[str]]:
        """
        Validate individual variant data integrity.

        Returns:
            (is_valid, error_message): Tuple indicating validation status
        """
        if not variant.chromosome:
            return False, f"Missing chromosome at index {idx}"

        if not variant.position:
            return False, f"Missing position at index {idx}"

        try:
            pos = int(variant.position)
            if pos < 0:
                return False, f"Negative position {pos} at index {idx}"
        except (ValueError, TypeError):
            return False, f"Invalid position '{variant.position}' at index {idx}"

        return True, None


# ============================================================================
# PROGRESS TRACKING
# ============================================================================

class OptimizedProgressTracker:
    """
    Memory-efficient progress bar with complexity-weighted ETA.
    Uses rolling window (deque) for O(1) memory instead of O(n).
    """

    def __init__(self, total: int, desc: str = "Processing"):
        self.total = total
        self.current = 0
        self.desc = desc
        self.start_time = time.time()
        self.complexity_window = deque(maxlen=100)

    def update(self, n: int = 1, complexity_factor: float = 1.0):
        """
        Update progress with complexity weighting for accurate ETA.

        Args:
            n: Number of items processed
            complexity_factor: Relative complexity (1.0 = avg, >1 = harder)
        """
        self.current += n
        self.complexity_window.append(complexity_factor)

        if self.current % 50 == 0 or self.current == self.total:
            self._display()

    def _display(self):
        """Display progress bar with complexity-weighted ETA."""
        pct = (self.current / self.total * 100) if self.total > 0 else 0
        elapsed = time.time() - self.start_time

        # Calculate ETA using rolling average complexity
        if len(self.complexity_window) > 10:
            avg_complexity = sum(self.complexity_window) / len(self.complexity_window)
            remaining_work = (self.total - self.current) * avg_complexity
            rate = self.current / elapsed if elapsed > 0 else 0
            eta = remaining_work / rate if rate > 0 else 0
        else:
            rate = self.current / elapsed if elapsed > 0 else 0
            eta = (self.total - self.current) / rate if rate > 0 else 0

        bar_len = 40
        filled = int(bar_len * pct / 100)
        bar = 'â–ˆ' * filled + '-' * (bar_len - filled)

        print(f"\r  {self.desc}: [{bar}] {pct:5.1f}% "
              f"({self.current:,}/{self.total:,}) | "
              f"{elapsed:.0f}s | ETA:{eta:.0f}s",
              end='', flush=True)

        if self.current == self.total:
            print()


# ============================================================================
# BATCH CLASSIFICATION
# ============================================================================

def classify_variants_production(
    matched_df: "pd.DataFrame",
    safeguard_config: Dict,
    clinvar_type: str = '',
    user_type: str = ''
) -> Tuple[List[VariantData], Dict]:
    """
    Classify batch of variants with production safeguards.

    Safeguards:
    - DataFrame validation
    - Per-variant validation
    - Automatic abort on high error rate (>5% default)
    - Detailed error logging
    - Memory-efficient progress tracking

    Args:
        matched_df: DataFrame of matched variants
        safeguard_config: Configuration dict with error thresholds
        clinvar_type: ClinVar file type (for logging)
        user_type: User data type (for logging)

    Returns:
        Tuple of (classified_variants_list, statistics_dict)

    Statistics dict keys:
        - total, classified, pathogenic, likely_pathogenic, vus,
          likely_benign, benign, conflicts
        - validation_errors: Count of data validation errors
        - errors: Count of processing errors

    Raises:
        ValidationError: If matched_df is invalid
        RuntimeError: If error rate exceeds threshold
    """
    import pandas as pd

    print("\n" + "="*70)
    print(f"[5/7] ðŸŽ¯ ACMG CLASSIFICATION (v{__version__})")
    print("="*70)

    # Validate DataFrame type
    if not isinstance(matched_df, pd.DataFrame):
        raise ValidationError(
            f"matched_df must be DataFrame, got {type(matched_df).__name__}",
            context={"stage": "classification"}
        )

    # Handle empty DataFrame
    if len(matched_df) == 0:
        logger.warning("matched_df is empty - no variants to classify")
        return [], {
            'total': 0, 'classified': 0, 'validation_errors': 0,
            'errors': 0, 'pathogenic': 0, 'likely_pathogenic': 0,
            'vus': 0, 'likely_benign': 0, 'benign': 0, 'conflicts': 0
        }

    logger.info(f"  âœ“ Validated DataFrame with {len(matched_df):,} variants")

    # Initialize components
    classifier = ACMGClassifier()
    normalizer = GenotypeNormalizer()

    # Initialize statistics
    stats = {
        'total': len(matched_df),
        'classified': 0,
        'pathogenic': 0,
        'likely_pathogenic': 0,
        'vus': 0,
        'likely_benign': 0,
        'benign': 0,
        'conflicts': 0,
        'validation_errors': 0,
        'errors': 0
    }

    classified_variants = []
    error_log = []
    progress = OptimizedProgressTracker(len(matched_df), "Classifying")

    # Process each variant (inlined from classify_single_variant)
    for idx, row in matched_df.iterrows():
        row_dict = row.to_dict()
        start_time = time.time()

        try:
            # Create variant object
            variant = VariantData(
                rsid=str(row_dict.get('rsid', '')),
                chromosome=str(row_dict.get('chromosome', '')),
                position=str(row_dict.get('position', '')),
                gene=str(row_dict.get('gene', '')),
                genotype=str(row_dict.get('genotype', '')) if 'genotype' in row_dict else '',
                clinical_sig=str(row_dict.get('clinical_sig', '')),
                review_status=str(row_dict.get('review_status', '')),
                molecular_consequence=str(row_dict.get('molecular_consequence', '')),
                variant_type=str(row_dict.get('variant_type', '')),
                num_submitters=int(row_dict.get('num_submitters', 0)) if 'num_submitters' in row_dict else 0
            )

            # Validate variant
            is_valid, error_msg = DataValidator.validate_variant(variant, idx)
            if not is_valid:
                stats['validation_errors'] += 1
                error_log.append({
                    'index': idx,
                    'variant_id': f"{variant.rsid}:{variant.chromosome}:{variant.position}",
                    'category': 'validation',
                    'error': error_msg
                })
                progress.update(1, complexity_factor=0.1)
                continue

            # Add allele information if available
            if 'ref_allele' in row_dict:
                variant.ref_allele = str(row_dict['ref_allele'])
            if 'alt_allele' in row_dict:
                variant.alt_allele = str(row_dict['alt_allele'])

            # Normalize genotype
            try:
                if variant.genotype and variant.chromosome:
                    normalized, gt_class = normalizer.normalize(
                        variant.genotype, variant.chromosome
                    )
                    variant.normalized_genotype = normalized
                    variant.genotype_class = gt_class
                    variant.zygosity = normalizer.get_zygosity(
                        variant.genotype, normalized, gt_class, variant.chromosome
                    )
                else:
                    variant.normalized_genotype = ''
                    variant.genotype_class = 'unknown'
                    variant.zygosity = 'unknown'
            except Exception as e:
                stats['errors'] += 1
                error_log.append({
                    'index': idx,
                    'variant_id': f"{variant.rsid}:{variant.chromosome}:{variant.position}",
                    'category': 'processing',
                    'error': f"Genotype normalization failed: {str(e)}"
                })
                progress.update(1, complexity_factor=0.1)
                continue

            # ACMG classification
            try:
                variant.star_rating = classifier.get_star_rating(variant.review_status)
                evidence = classifier.assign_evidence(variant)
                variant.acmg_evidence = evidence
                classification, confidence = classifier.combine_evidence(evidence)
                variant.acmg_classification = classification
                variant.confidence_level = confidence
            except Exception as e:
                stats['errors'] += 1
                error_log.append({
                    'index': idx,
                    'variant_id': f"{variant.rsid}:{variant.chromosome}:{variant.position}",
                    'category': 'processing',
                    'error': f"ACMG classification failed: {str(e)}"
                })
                progress.update(1, complexity_factor=0.1)
                continue

            # Update statistics
            stats['classified'] += 1

            if variant.acmg_classification == 'Pathogenic':
                stats['pathogenic'] += 1
            elif variant.acmg_classification == 'Likely Pathogenic':
                stats['likely_pathogenic'] += 1
            elif variant.acmg_classification == 'Uncertain Significance':
                stats['vus'] += 1
            elif variant.acmg_classification == 'Likely Benign':
                stats['likely_benign'] += 1
            elif variant.acmg_classification == 'Benign':
                stats['benign'] += 1

            if variant.acmg_evidence and variant.acmg_evidence.has_conflict():
                stats['conflicts'] += 1

            classified_variants.append(variant)

            complexity = time.time() - start_time
            progress.update(1, complexity_factor=min(complexity * 10, 5.0))

        except Exception as e:
            stats['errors'] += 1
            error_log.append({
                'index': idx,
                'variant_id': f"{row_dict.get('rsid', 'unknown')}:{row_dict.get('chromosome', '?')}:{row_dict.get('position', '?')}",
                'category': 'processing',
                'error': str(e)
            })
            progress.update(1, complexity_factor=0.1)

    # Display results
    print(f"\n  âœ“ Classification complete")
    print(f"    â€¢ Total: {stats['total']:,}")
    print(f"    â€¢ Classified: {stats['classified']:,}")
    print(f"    â€¢ Pathogenic: {stats['pathogenic']:,}")
    print(f"    â€¢ Likely Pathogenic: {stats['likely_pathogenic']:,}")
    print(f"    â€¢ VUS: {stats['vus']:,}")
    print(f"    â€¢ Likely Benign: {stats['likely_benign']:,}")
    print(f"    â€¢ Benign: {stats['benign']:,}")
    print(f"    â€¢ Conflicts: {stats['conflicts']:,}")

    if stats['validation_errors'] > 0:
        print(f"    âš ï¸ Validation Errors: {stats['validation_errors']:,}")
    if stats['errors'] > 0:
        print(f"    âš ï¸ Processing Errors: {stats['errors']:,}")

    # Save error log
    if error_log:
        error_file = Path(safeguard_config['error_log_path'])
        error_file.parent.mkdir(parents=True, exist_ok=True)
        with open(error_file, 'w') as f:
            f.write(f"VARIANT CLASSIFICATION ERRORS (v{__version__})\n")
            f.write("="*70 + "\n\n")
            for err in error_log:
                f.write(f"Index: {err['index']}\n")
                f.write(f"Variant: {err['variant_id']}\n")
                f.write(f"Category: {err['category']}\n")
                f.write(f"Error: {err['error']}\n")
                f.write("-"*70 + "\n")
        print(f"    ðŸ“„ Error details: {error_file}")

    # Check error rate threshold
    total_errors = stats['validation_errors'] + stats['errors']
    error_rate = total_errors / stats['total'] if stats['total'] > 0 else 0

    if safeguard_config.get('abort_on_threshold', True):
        if error_rate > safeguard_config['max_error_rate']:
            raise RuntimeError(
                f"ABORT: Error rate {error_rate*100:.1f}% exceeds "
                f"{safeguard_config['max_error_rate']*100:.0f}% threshold. "
                f"Check error log: {error_file if error_log else 'N/A'}"
            )
    elif error_rate > safeguard_config['max_error_rate']:
        logger.warning(
            f"High error rate {error_rate*100:.1f}% (graceful mode, continuing)"
        )

    return classified_variants, stats


# ============================================================================
# SELF-TEST
# ============================================================================

if __name__ == "__main__":
    print("="*70)
    print(f"varidex/utils/helpers.py - Self-Test v{__version__}")
    print("="*70)
    print("\nâœ… Module loaded successfully")
    print(f"âœ… DataValidator class available")
    print(f"âœ… OptimizedProgressTracker class available")
    print(f"âœ… classify_variants_production() function available")
    print(f"\nðŸ”§ CHANGES FROM v6.0.1:")
    print(f"   â€¢ VERSION: Unified to v{__version__}")
    print(f"   â€¢ REMOVED: ErrorCode enum (moved to exceptions.py)")
    print(f"   â€¢ REMOVED: ErrorInfo class (moved to exceptions.py)")
    print(f"   â€¢ INLINED: classify_single_variant into classify_variants_production")
    print(f"   â€¢ IMPORTS: Direct imports (not varidex._imports)")
    print(f"\nðŸš€ Ready for production deployment!")
